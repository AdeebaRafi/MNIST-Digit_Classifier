# -*- coding: utf-8 -*-
"""MNIST_Digits_project_demo

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/scratchpad/mnist-digits-project-demo.5d4c1130-2f32-40c5-b565-f9f6586288dc.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250921/auto/storage/goog4_request%26X-Goog-Date%3D20250921T080026Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D14ef2f79697b77aaa01ca8fcd6cd87852d066bd8e1368228025885ba35bc6d01c8d74465c9cd14af5b99b0e57ed046db219c566a867b25439d47fb7d89c400f7662f3f6c8d376dc9fc641e7ef82c966150961933babb1b981ff6434c8b90bd5d55c16b9d7b585b6725f5b38d29bcc45dd5473ba0a41137a35a969ddb651d78d19cfdcf781207e7f6182bb18b868b65b742c2a620cacc762e1f7fb97284d5cf24b530e5e92b015e190f6fb35b46247d7cf6cfd80f98b68f9654a69d9586834792c7c637f869258d7d31a03758c68e07f48e55c8078c0e078460f5f245907f00a76c8927a30a8c30812f566b11126361b27903f72c9eaf92bbc51035fea47ed497

# Introduction to Deep Learning in Python
  
In this notebook, we’ll explore the basics of deep learning using **Python + Keras**.  
By the end, you’ll understand how neural networks work and build your first **digit classifier** using the MNIST dataset.

---
"""

# --- SETUP ---

# Install TensorFlow if not already present
!pip install tensorflow matplotlib

# Imports
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

"""## Step 1 – Setup

We’ll start by installing required libraries:
- **TensorFlow** → to build and train neural networks
- **Matplotlib** → for visualization


"""

# Example: Sigmoid activation
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.linspace(-10, 10, 100)
y = sigmoid(x)

plt.plot(x, y)
plt.title("Sigmoid Activation Function")
plt.xlabel("Input")
plt.ylabel("Output")
plt.grid(True)
plt.show()

"""## Step 2 – Neural Networks Basics

- A **neural network** is made up of layers of connected “neurons”.
- Each neuron applies a **mathematical function** to its inputs.
- **Activation functions** introduce non-linearity, helping the network learn complex patterns.

Here, we’ll start with a simple **Sigmoid activation function** to see how it behaves.


"""

# Simple feedforward neural network with random data
model = Sequential([
    Dense(8, activation='relu', input_shape=(10,)),
    Dense(1, activation='sigmoid')
])

model.summary()

"""## Step 3 – Building a Simple Model with Keras

- We’ll create a **feedforward neural network** using Keras.
- **Sequential model** = stack of layers.
- Example architecture:
  - Input: 10 features
  - Hidden: 8 neurons with ReLU
  - Output: 1 neuron with Sigmoid (binary classification)

Let’s see how Keras makes this easy.

"""

# Train simple model on dummy data
X = np.random.rand(100, 10)
y = np.random.randint(0, 2, 100)

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(X, y, epochs=5, batch_size=8)

"""## Step 4 – Mini Project: MNIST Handwritten Digits

The **MNIST dataset** contains 70,000 handwritten digits (0–9).

- Each image is **28×28 pixels**.
- Goal: Build a neural network to classify which digit it is.
- This is the "Hello World" of deep learning!

First, let’s load and prepare the data.


"""

# Load MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Normalize and flatten
X_train = X_train.reshape(60000, 784).astype("float32") / 255
X_test = X_test.reshape(10000, 784).astype("float32") / 255

# One-hot encode labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

print("Training data shape:", X_train.shape)
print("Test data shape:", X_test.shape)

"""Let’s look at a few examples from the dataset.


"""

# Visualize sample digits
plt.figure(figsize=(6,6))
for i in range(9):
    plt.subplot(3,3,i+1)
    plt.imshow(X_train[i].reshape(28,28), cmap="gray")
    plt.title(f"Label: {np.argmax(y_train[i])}")
    plt.axis("off")
plt.show()

"""## Step 5 – Training the Model

We’ll build a **3-layer neural network**:
- Input layer: 784 inputs (flattened image)
- Hidden layer 1: 64 neurons (ReLU)
- Hidden layer 2: 64 neurons (ReLU)
- Output layer: 10 neurons (Softmax, one for each digit)

Optimizer: Adam  
Loss: Categorical Crossentropy  


"""

# Build model
mnist_model = Sequential([
    Dense(64, activation='relu', input_shape=(784,)),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile
mnist_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train
history = mnist_model.fit(X_train, y_train, validation_split=0.2, epochs=5, batch_size=32)

"""## Step 6 – Evaluate Performance

Now, let’s check how well our model performs on the **test set** (unseen data).  
We’ll also plot the training vs validation accuracy to see the learning progress.


"""

# Evaluate on test set
test_loss, test_acc = mnist_model.evaluate(X_test, y_test)
print("Test Accuracy:", test_acc)

# Plot training history
plt.plot(history.history['accuracy'], label='train accuracy')
plt.plot(history.history['val_accuracy'], label='val accuracy')
plt.title("Training vs Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""## Step 7 – Making Predictions

Finally, let’s see what the model predicts for a few test digits.


"""

# Predict first 5 test samples
predictions = mnist_model.predict(X_test[:5])

for i in range(5):
    plt.imshow(X_test[i].reshape(28,28), cmap="gray")
    plt.title(f"Predicted: {np.argmax(predictions[i])}, True: {np.argmax(y_test[i])}")
    plt.axis("off")
    plt.show()